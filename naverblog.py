import time
from urllib import parse
import urllib.request
from multiprocessing import Pool

from bs4 import BeautifulSoup
import re
import PostgresqlDBConnent
import MorphAnal
import json
from PgHelper import PgHelper

# text ì •ì œí•˜ê¸°


def clean_text(text):
    # content = text.get_text()
    content = text
    cleaned_text = re.sub('[a-zA-Z]', '', content)
    cleaned_text = re.sub('[\{\}\[\]\/?.,;:|\)*~`!^\-_+<>â–¶â–½â™¡â—€â”@\#$%&\\\=\(\'\"â“’(\n)(\t)]', '', cleaned_text)
    cleaned_text = cleaned_text.replace("ğŸ‡²\u200bğŸ‡®\u200bğŸ‡±\u200bğŸ‡±\u200bğŸ‡®\u200bğŸ‡ª\u200b", "")
    cleaned_text = cleaned_text.replace("ì˜¤ë¥˜ë¥¼ ìš°íšŒí•˜ê¸° ìœ„í•œ í•¨ìˆ˜ ì¶”ê°€ ", "")
    cleaned_text = cleaned_text.replace("ë™ì˜ìƒ ë‰´ìŠ¤ ì˜¤ë¥˜ë¥¼ ìš°íšŒí•˜ê¸° ìœ„í•œ í•¨ìˆ˜ ì¶”ê°€ ", "")
    cleaned_text = cleaned_text.replace("ë¬´ë‹¨ì „ì¬ ë° ì¬ë°°í¬ ê¸ˆì§€", "")
    cleaned_text = cleaned_text.strip('\n')
    return cleaned_text


def blog_crawling(crawling_url):
    print(crawling_url[0])
    # print(crawling_url[1])
    try:
        req = urllib.request.urlopen(crawling_url[0])
        res = req.read()
        soup = BeautifulSoup(res, 'html.parser')

        try:
            # keywords = soup.findAll('div', {'class': 'se_textView'})
            # keywords = soup.findAll('div', {'class': 'se-main-container'})
            # keywords = soup.findAll('div', {'class': 'se-module se-module-text'})
            keywords = soup.find_all('span', {'class': 'se-fs- se-ff- '})
            # print(len(keywords))
            # print(1)
            if len(keywords) > 0:
                for a in keywords:
                    text = clean_text(a.get_text())
                    if len(text) > 0:
                        # print('\t' + text)
                        PostgresqlDBConnent.update_raw_data(text, crawling_url[1])
                    # print(keywords.get_text(strip=True))
                    # MorphAnal.add_data(keywords.get_text(strip=True))
            else:
                keywords = soup.find_all('div', {'id': 'postViewArea'})
                # print('2')
                if len(keywords) > 0:
                    for a in keywords:
                        # text = a.get_text()
                        text = clean_text(a.get_text())
                        if len(text) > 0:
                            # print(text)
                            PostgresqlDBConnent.update_raw_data(text, crawling_url[1])
                else:
                    keywords = soup.findAll('div', {'class': 'se_textView'})
                    # print('3')
                    if len(keywords) > 0:
                        # print(keywords.get_text(strip=True))
                        # print(keywords)
                        for a in keywords:
                            # text = a.get_text()
                            text = clean_text(a.get_text())
                            if len(text) > 0:
                                # print('\t' + text)
                                PostgresqlDBConnent.update_raw_data(text, crawling_url[1])
                    else:
                        print('xxxxxxxxxxxxxxxx')
        except Exception as err:
            print('??????', err)

    except urllib.error.HTTPError as e:
        # err = e.read()
        code = e.getcode()
        print(code)


def search_keyword(pghelper: object, keyword: object) -> object:
    client_id = "7o0ZnBCQzfLEleVV5JDM"
    client_secret = "uHZM6Mq0hE"
    start = 1
    display = 100
    enc_text = parse.quote(keyword)

    while start <= 1000:
        url = "https://openapi.naver.com/v1/search/blog?query=" + enc_text + "&display=" \
                      + str(display) + "&start=" + str(start)  # json ê²°ê³¼
        # url = "https://openapi.naver.com/v1/search/blog.xml?query=" + encText # xml ê²°ê³¼
        request = urllib.request.Request(url)
        request.add_header("X-Naver-Client-Id", client_id)
        request.add_header("X-Naver-Client-Secret", client_secret)
        response = urllib.request.urlopen(request)
        resp_code = response.getcode()

        if resp_code == 200:
            response_body = response.read()
            # print(response_body.decode('utf-8'))
            jsonobject = json.loads(response_body)
            jsonarray = jsonobject.get("items")
            print("\tTotal : ", jsonobject.get("total"))
            print("\tStart : ", jsonobject.get("start"))
            if jsonobject.get("total") == 0:
                break
            else:
                # PostgresqlDBConnent.keyword_add_db(jsonarray)
                if len(jsonarray) > 0:
                    pghelper.keyword_add_db(jsonarray)
                else:
                    print("xxxxx")
        else:
            print("Error Code:" + str(resp_code))

        start = start + display


if __name__ == '__main__':
    print("Search_Keyword call")
    # Search_Keyword("ê°•ì›ë„")
    bloglink = []
    rawdata = []
    tour_api_keyword = []

    start_time = time.time()
    # kangwong = ['ì¶˜ì²œ ë§›ì§‘', 'ì›ì£¼ ë§›ì§‘', 'ê°•ë¦‰ ë§›ì§‘', 'ë™í•´ ë§›ì§‘', 'íƒœë°± ë§›ì§‘', 'ì†ì´ˆ ë§›ì§‘', 'ì‚¼ì²™ ë§›ì§‘', 'í™ì²œ ë§›ì§‘', 'íš¡ì„± ë§›ì§‘',
    #             'ì˜ì›” ë§›ì§‘', 'í‰ì°½ ë§›ì§‘', 'ì •ì„  ë§›ì§‘', 'ì² ì› ë§›ì§‘', 'í™”ì²œ ë§›ì§‘', 'ì–‘êµ¬ ë§›ì§‘', 'ì¸ì œ ë§›ì§‘', 'ê³ ì„± ë§›ì§‘', 'ì–‘ì–‘ ë§›ì§‘']

    # pghelper = PgHelper()

    # pghelper.get_keyword_data(tour_api_keyword)
    # # print(tour_api_keyword.index('[ê°•ë¦‰ ë°”ìš°ê¸¸ 11êµ¬ê°„] ì‹ ì‚¬ì„ë‹¹ê¸¸'))
    # for i in tour_api_keyword[3214:]:
    #     print(i)
    #     Search_Keyword(pghelper, i)

    # for i in kangwong:
    #     print(i)
    #     Search_Keyword(pghelper, i)

    # pghelper.get_blog_link(bloglink)
    # pghelper.CloseDB()
    # url1 ="https://blog.naver.com/PostView.nhn?blogId=soapdiary&logNo=222087156559"
    # url2 ="https://blog.naver.com/PostView.nhn?blogId=soapdiary&logNo=222087156559"
    # bloglink.append([url1, url2])
    # for i in bloglink:
    #     Blog_crawling(i)

    # pool = Pool(processes=8)  # 4ê°œì˜ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    # pool.map(Blog_crawling, bloglink)  # get_contetn í•¨ìˆ˜ë¥¼ ë„£ì–´ì¤ì‹œë‹¤.

    print("--- %s seconds ---" % (time.time() - start_time))
    PostgresqlDBConnent.get_raw_data(rawdata)
    #
    # print(rawdata)
    # print(len(rawdata))
    MorphAnal.process_data(rawdata)
    print("--- %s seconds ---" % (time.time() - start_time))
    # PostgresqlDBConnent.update_raw_data('111asdfasdfasdf',
    # 'https://blog.naver.com/eye4y?Redirect=Log&logNo=222091294214')
    # PostgresqlDBConnent.blog_add_db()
    # Blog_crawing("https://blog.naver.com/PostView.nhn?blogId=d2house&1364")
    # Blog_crawing("https://withbeatles.tistory.com/3544")
    print("Search_Keyword return")
